{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb2e1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa6d91dd830>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from dataset import*\n",
    "from synthetic_concept_model import *\n",
    "from synthetic_coop_model import *\n",
    "from baselines import *\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "import torch\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "random.seed(7)\n",
    "numpy.random.seed(seed=7)\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "479b8138-eebc-41d8-a4a6-bf778329e14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !nvidia-smi\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bf0d59e-9cf0-4ef8-aa0a-04b36579f652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__CUDNN VERSION: 8902\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: Tesla V100-SXM2-32GB\n",
      "__CUDA Device Total Memory [GB]: 34.079637504\n"
     ]
    }
   ],
   "source": [
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454ff7e",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb3a5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim_info = dict()\n",
    "label_dim_info = dict()\n",
    "transform_dim = 100000 #100000\n",
    "\n",
    "intersections = get_intersections(num_modalities=2)\n",
    "\n",
    "feature_dim_info['12'] = 10\n",
    "feature_dim_info['1'] = 6\n",
    "feature_dim_info['2'] = 6\n",
    "\n",
    "label_dim_info['12'] = 10\n",
    "label_dim_info['1'] = 6\n",
    "label_dim_info['2'] = 6\n",
    "num_concepts = 1\n",
    "transforms_2concept = None\n",
    "transforms_2hd = None\n",
    "num_data = 10000\n",
    "noise=0.3\n",
    "pos_prob=0.5\n",
    "# total_data, total_labels, total_concepts, total_raw_features = generate_data_concepts(num_data, num_concepts,\n",
    "#                                                                                       feature_dim_info,\n",
    "#                                                                                       label_dim_info,\n",
    "#                                                                                       transform_dim=transform_dim,\n",
    "#                                                                                      noise=noise,\n",
    "#                                                                                      pos_prob=pos_prob)\n",
    "\n",
    "# synth_data_dict = {'total_data':total_data, 'total_labels':total_labels, 'total_concepts':total_concepts, 'total_raw_features':total_raw_features}\n",
    "# synth_data_file_name = '../synth_data/'+'synth_data_exp2_'+str(noise)+'_'+str(pos_prob)+'.pkl'\n",
    "# pickle.dump(synth_data_dict, open(synth_data_file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "968974b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting & loading\n",
    "synth_data_dict = pickle.load(open('../synth_data/'+'synth_data_exp2_'+str(noise)+'_'+str(pos_prob)+'.pkl', 'rb'))\n",
    "total_data = synth_data_dict['total_data']\n",
    "total_labels = synth_data_dict['total_labels']\n",
    "total_concepts = synth_data_dict['total_concepts']\n",
    "total_raw_features = synth_data_dict['total_raw_features']\n",
    "\n",
    "dataset = MultiConcept(total_data, total_labels, total_concepts, 0)\n",
    "batch_size = 100\n",
    "pretrain_dataset, finetune_dataset = torch.utils.data.random_split(dataset,  \n",
    "                                                            [int(0.8 * num_data), num_data - int(0.8 * num_data)])\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(pretrain_dataset,\n",
    "                                                           [int(0.8 * len(pretrain_dataset)), len(pretrain_dataset) - int(0.8 * len(pretrain_dataset))])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True,\n",
    "                          batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4931e5bc-73e0-4470-86e8-3df0d2a5240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrain_dataset), len(finetune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4059c6-b3f0-4fc7-a4c4-291f4e518a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_dim = 101024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31d4dc",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801734b-005d-4068-bbe0-8f19021e0c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scenario 1\n",
    "Pretrain on a large dataset $D_L:=\\{x_i,c_i\\}$ by minimizing InfoNCE_CLUB between $(Z_{\\bar{c}},c|x)$ and fine-tune on small dataset $D_S:=\\{x_j,c_j,y_j\\}$ with supervised learning $x \\rightarrow Z_{\\bar{c}}, c \\rightarrow y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ce0ace-a2be-4652-bbc3-c167566d697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptCLSUP_Pretrain(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim, embed_dim, layers=2, activation='relu', lr=1e-4):\n",
    "        super(ConceptCLSUP_Pretrain, self).__init__()\n",
    "        self.critic_hidden_dim = 512\n",
    "        self.critic_layers = 1\n",
    "        self.critic_activation = 'relu'\n",
    "        self.lr = lr\n",
    "\n",
    "        # encoders\n",
    "        self.backbone = mlp(x_dim, hidden_dim, embed_dim, layers, activation)\n",
    "        self.linears_infonce = mlp(embed_dim, embed_dim, embed_dim, 1, activation) \n",
    "\n",
    "        # critics\n",
    "        concept_dim = 1\n",
    "        self.club_critic = CLUBInfoNCECritic(embed_dim + x_dim, concept_dim, self.critic_hidden_dim, self.critic_layers, self.critic_activation)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # compute embedding\n",
    "        z = self.linears_infonce(self.backbone(x))\n",
    "        # compute critic scores\n",
    "        club_infonce_score = self.club_critic(torch.cat([z, x], dim=-1), c)\n",
    "        return club_infonce_score\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def get_backbone(self):\n",
    "        return self.backbone\n",
    "    \n",
    "\n",
    "def train_concept_informed_Pretrain_model(concept_encoder, model, train_loader,val_loader, num_epochs, device, lr, log_interval,\n",
    "                          save_interval, save_path):\n",
    "\n",
    "    concept_encoder.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_err = torch.tensor(1e7)\n",
    "    tepoch = tqdm(range(num_epochs))\n",
    "    for epoch in tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        for batch_idx, (data, concept, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            c, z_c = concept_encoder(data)\n",
    "            loss = model(data, c) #z_c\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tepoch.set_postfix(loss=loss.item())\n",
    "        if epoch % save_interval == 0:\n",
    "            val_err = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, concept, target) in enumerate(val_loader):\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    c , z_c = concept_encoder(data)\n",
    "                    output = model(data, c) #z_c\n",
    "                    val_err += output\n",
    "                val_err = val_err / len(val_loader)\n",
    "            if val_err < best_val_err:\n",
    "                best_val_err = val_err\n",
    "\n",
    "            else:\n",
    "                print('Val loss did not improve')\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'concept_informed_model.pth'))\n",
    "                return model\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e160d497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 512\n",
    "embed_dim = 50 \n",
    "concept_encoder = ConceptEncoder(transform_dim, embed_dim, 1, hidden_dim, layers=1).to(device)\n",
    "model = ConceptCLSUP_Pretrain(transform_dim, hidden_dim, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f646758-6c87-4b52-9aa8-c7880afe083e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42:   4%|▍         | 42/1000 [03:47<1:15:54,  4.75s/it, loss=1.37e-8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42:   4%|▍         | 42/1000 [03:48<1:26:52,  5.44s/it, loss=1.37e-8]\n",
      "Epoch 573:  57%|█████▋    | 573/1000 [3:07:07<2:17:31, 19.32s/it, loss=-6.39e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 573:  57%|█████▋    | 573/1000 [3:07:10<2:19:28, 19.60s/it, loss=-6.39e+3]\n"
     ]
    }
   ],
   "source": [
    "# train concpet encoder \n",
    "trained_concept_encoder = train_concept_encoder(concept_encoder, train_loader,val_loader, 1000, device, 1e-5, 1e-5, 25, 3, '../trained_models')\n",
    "# train concept informed model\n",
    "trained_concept_informed_Pretrain_model = train_concept_informed_Pretrain_model(trained_concept_encoder, model, train_loader, val_loader, 1000, device, 1e-5, 25, 3, '../trained_models')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f1468f4-be6c-4135-af1a-6e8ead8d53cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098b2906-125a-4b67-8888-58e81d96ead7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:   2%|▏         | 18/1000 [01:33<1:17:12,  4.72s/it, loss=3.53e-7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:   2%|▏         | 18/1000 [01:35<1:26:56,  5.31s/it, loss=3.53e-7]\n",
      "Epoch 612:  61%|██████    | 612/1000 [3:19:51<2:04:56, 19.32s/it, loss=-8.38e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 612:  61%|██████    | 612/1000 [3:19:54<2:06:44, 19.60s/it, loss=-8.38e+3]\n"
     ]
    }
   ],
   "source": [
    "# train concpet encoder \n",
    "trained_concept_encoder = train_concept_encoder(concept_encoder, train_loader,val_loader, 1000, device, 1e-5, 1e-5, 25, 3, '../trained_models')\n",
    "# train concept informed model\n",
    "trained_concept_informed_Pretrain_model = train_concept_informed_Pretrain_model(trained_concept_encoder, model, train_loader, val_loader, 1000, device, 1e-5, 25, 3, '../trained_models')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d932e1-dcea-45ef-aad5-04eda8cd4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptCLSUP_Finetune_Sc1(nn.Module):\n",
    "    def __init__(self, backbone, embed_dim, hidden_dim, **extra_kwargs):\n",
    "        super(ConceptCLSUP_Finetune_Sc1, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # self.concept_encoder = mlp(1, hidden_dim, 128, 1, 'relu')\n",
    "        self.fc = nn.Linear(embed_dim+1, 1)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x = self.backbone(x)\n",
    "        x_c = torch.cat((x, c), dim=1)\n",
    "        out = self.fc(x_c)\n",
    "        return out\n",
    "def train_fine_tune_model_sc1(model, train_loader, val_loader, num_epochs, lr, weight_decay, device, log_interval,\n",
    "                          save_interval):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_err = torch.tensor(1e7)\n",
    "    loss_func = nn.BCELoss() #nn.CrossEntropyLoss() #\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    tepoch = tqdm(range(num_epochs))\n",
    "    for epoch in tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "        for batch_idx, (data, concept, target) in enumerate(train_loader):\n",
    "            data, concept, target = data.to(device), concept.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(data, concept)\n",
    "            preds =  torch.sigmoid(logits) #torch.softmax(logits, dim=-1)\n",
    "            loss = loss_func(preds, target.float()) #F.cross_entropy(preds, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tepoch.set_postfix(loss=loss.item())\n",
    "        if epoch % save_interval == 0:\n",
    "            val_err = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, concept, target) in enumerate(val_loader):\n",
    "                    data, concept, target = data.to(device), concept.to(device), target.to(device)\n",
    "                    logits = model(data, concept)\n",
    "                    preds = torch.sigmoid(logits)\n",
    "                    val_err += loss_func(preds, target.float())\n",
    "                val_err = val_err / len(val_loader)\n",
    "            # print('Val loss: {:.6f}'.format(val_err))\n",
    "            if val_err < best_val_err:\n",
    "                best_val_err = val_err\n",
    "            else:\n",
    "                print('Val loss did not improve')\n",
    "                return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738cfac9-dd11-4960-a047-199455cece76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#New test split\n",
    "num_data = len(finetune_dataset)\n",
    "batch_size = 100\n",
    "new_trainval_dataset, new_test_dataset = torch.utils.data.random_split(finetune_dataset,  \n",
    "                                                            [int(0.5 * num_data), num_data - int(0.5 * num_data)])\n",
    "new_train_dataset, new_val_dataset = torch.utils.data.random_split(new_trainval_dataset,\n",
    "                                                           [int(0.8 * len(new_trainval_dataset)), len(new_trainval_dataset) - int(0.8 * len(new_trainval_dataset))])\n",
    "\n",
    "new_train_loader = DataLoader(new_train_dataset, shuffle=True, drop_last=True,\n",
    "                          batch_size=batch_size)\n",
    "new_val_loader = DataLoader(new_val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "new_test_loader = DataLoader(new_test_dataset, shuffle=False, drop_last=False)\n",
    "\n",
    "#Train Final Model\n",
    "\n",
    "backbone = trained_concept_informed_Pretrain_model.get_backbone()\n",
    "# new_model = nn.Sequential(backbone, mlp(50, 256, 1, 1, activation= 'relu'))\n",
    "new_model = ConceptCLSUP_Finetune_Sc1(backbone, embed_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d76b41-d6d2-44fe-8b76-69d675a988e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [02:31<05:54,  1.98it/s, loss=0.000155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy: 0.861\n",
      "Precision: 0.8188539741219963\n",
      "Recall: 0.9152892561983471\n",
      "F1-score: 0.8643902439024391\n"
     ]
    }
   ],
   "source": [
    "from baselines import*\n",
    "\n",
    "# final_model = mlp_train(new_model, new_train_loader, new_val_loader, 100, 1e-5, 1e-5,'cuda', 100, 100)\n",
    "final_model = train_fine_tune_model_sc1(new_model, new_train_loader, new_val_loader, 1000, 1e-5, 1e-5,'cuda', 100, 100)\n",
    "\n",
    "test_embeds = torch.stack([sample[0] for sample in  new_test_dataset])#.detach().cpu().numpy()\n",
    "test_concepts = torch.tensor([sample[1].item() for sample in  new_test_dataset]).unsqueeze(1)\n",
    "test_labels = np.array([sample[-1].item() for sample in  new_test_dataset])\n",
    "    \n",
    "out = final_model(test_embeds.to(device), test_concepts.to(device))\n",
    "predictions = torch.sigmoid(out).round().detach().cpu().numpy()\n",
    "        \n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0c20c-e369-45fc-a87f-e68c325cc0c2",
   "metadata": {},
   "source": [
    "## Scenario 2\n",
    "Pretrain on a large dataset $D_L:=\\{x_i,c_i\\}$ by minimizing InfoNCE_CLUB between $(Z_{\\bar{c}},c|x)$\n",
    "Proceed to fine-tune on small dataset $D_S:=\\{x_j,c_j, y_j\\}$ with both supervised learning and InfoNCE_CLUB score \n",
    "$\\mathop{\\arg \\min}\\limits_{\\theta , \\phi} \\mathcal{L}\\bigl( y, f_{\\theta}(Z_{\\bar{c}},c)\\bigr) + \\lambda Info_{NCE\\_CLUB} \\bigl( Z_{\\bar{c}},c|x \\bigr)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6904ea6-0ba8-48ad-b6ad-27cfc2f35eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptCLSUP_Pretrain_Sc2(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim, embed_dim, layers=2, activation='relu', lr=1e-4, concept_dim=1):\n",
    "        super(ConceptCLSUP_Pretrain_Sc2, self).__init__()\n",
    "        self.critic_hidden_dim = 512\n",
    "        self.critic_layers = 1\n",
    "        self.critic_activation = 'relu'\n",
    "        self.lr = lr\n",
    "\n",
    "        # encoders\n",
    "        self.backbone = mlp(x_dim, hidden_dim, embed_dim, layers, activation)\n",
    "        self.linears_club = mlp(embed_dim, embed_dim, embed_dim, 1, activation) \n",
    "\n",
    "        # critics\n",
    "        self.club_critic = CLUBInfoNCECritic(embed_dim + x_dim, concept_dim, self.critic_hidden_dim, self.critic_layers, self.critic_activation)\n",
    "        \n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # compute embedding\n",
    "        z_comp = self.linears_club(self.backbone(x))\n",
    "        # compute critic scores\n",
    "        club_infonce_score = self.club_critic(torch.cat([z_comp, x], dim=-1), c)\n",
    "        \n",
    "        return  club_infonce_score #+ infonce_score\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        #what should the embedding be?\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def get_backbone(self):\n",
    "        return self.backbone #, self.linears_club\n",
    "\n",
    "    \n",
    "class ConceptCLSUP_Finetune_Sc2(nn.Module):\n",
    "    def __init__(self, backbone, x_dim, hidden_dim, embed_dim, layers=2, activation='relu', lr=1e-4, concept_dim=1):\n",
    "        super(ConceptCLSUP_Finetune_Sc2, self).__init__()\n",
    "        self.critic_hidden_dim = 512\n",
    "        self.critic_layers = 1\n",
    "        self.critic_activation = 'relu'\n",
    "        self.lr = lr\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # encoders\n",
    "        self.lable_encoder = mlp(embed_dim, embed_dim, embed_dim, 1, activation) \n",
    "        self.linears_club = mlp(embed_dim, embed_dim, embed_dim, 1, activation) \n",
    "        self.linears_label = mlp(embed_dim + embed_dim + 1, embed_dim, 1, 1, activation)\n",
    "\n",
    "        # critics\n",
    "        self.club_critic = CLUBInfoNCECritic(embed_dim + x_dim, concept_dim, self.critic_hidden_dim, self.critic_layers, self.critic_activation)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # compute embedding\n",
    "        z_comp = self.linears_club(self.backbone(x))\n",
    "        z_y = self.lable_encoder(self.backbone(x))\n",
    "        logit = self.linears_label(torch.cat([z_comp, z_y, c], dim=-1))\n",
    "        \n",
    "        # compute critic scores\n",
    "        club_infonce_score = self.club_critic(torch.cat([z_comp, x], dim=-1), c)\n",
    "        \n",
    "        return  club_infonce_score, logit\n",
    "\n",
    "    def get_logits(self, x, c):\n",
    "        z_comp = self.linears_club(self.backbone(x))\n",
    "        z_y = self.lable_encoder(self.backbone(x))\n",
    "        logit = self.linear_label(torch.cat([z_comp, z_y, c], dim=-1))\n",
    "        return logit\n",
    "\n",
    "def train_fine_tune_model_sc2(model, train_loader, val_loader, num_epochs, lr, weight_decay, device, log_interval,\n",
    "                          save_interval):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_err = torch.tensor(1e7)\n",
    "    loss_func = nn.BCELoss() #nn.CrossEntropyLoss() #\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    tepoch = tqdm(range(num_epochs))\n",
    "    for epoch in tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "        for batch_idx, (data, concept, target) in enumerate(train_loader):\n",
    "            data, concept, target = data.to(device), concept.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            score, logits = model(data, concept)\n",
    "            preds =  torch.sigmoid(logits) #torch.softmax(logits, dim=-1)\n",
    "            label_loss = loss_func(preds, target.float()) #F.cross_entropy(preds, target)\n",
    "            ###########??????????????????????????????????????????? lamb\n",
    "            lamb = 0.5\n",
    "            loss = lamb *score + label_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "        tepoch.set_postfix(loss=loss.item())\n",
    "        if epoch % save_interval == 0:\n",
    "            val_err = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, concept, target) in enumerate(val_loader):\n",
    "                    data, concept, target = data.to(device), concept.to(device), target.to(device)\n",
    "                    score, logits = model(data, concept)\n",
    "                    preds = torch.sigmoid(logits)\n",
    "                    output = score + loss_func(preds, target.float())\n",
    "                    val_err += output\n",
    "                val_err = val_err / len(val_loader)\n",
    "            # print('Val loss: {:.6f}'.format(val_err))\n",
    "            if val_err < best_val_err:\n",
    "                best_val_err = val_err\n",
    "            else:\n",
    "                print('Val loss did not improve')\n",
    "                return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5b6aa1-aa0b-4f1b-860a-7bc0dbdcaf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 512\n",
    "embed_dim = 50 \n",
    "concept_encoder = ConceptEncoder(transform_dim, embed_dim, 1, hidden_dim, layers=1).to(device)\n",
    "model = ConceptCLSUP_Pretrain_Sc2(transform_dim, hidden_dim, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e26db17-db3b-412e-be0b-200b69cbf43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21:   2%|▏         | 21/1000 [01:46<1:15:39,  4.64s/it, loss=9.91e-8] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21:   2%|▏         | 21/1000 [01:47<1:23:45,  5.13s/it, loss=9.91e-8]\n",
      "Epoch 672:  67%|██████▋   | 672/1000 [3:39:21<1:45:41, 19.33s/it, loss=-9.37e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 672:  67%|██████▋   | 672/1000 [3:39:24<1:47:05, 19.59s/it, loss=-9.37e+3]\n"
     ]
    }
   ],
   "source": [
    "# train concpet encoder\n",
    "trained_concept_encoder = train_concept_encoder(concept_encoder, train_loader,val_loader, 1000, device, 1e-5, 1e-5, 25, 3, '../trained_models')\n",
    "# train concept informed model\n",
    "trained_concept_informed_Pretrain_model = train_concept_informed_Pretrain_model(trained_concept_encoder, model, train_loader, val_loader, 1000, device, 1e-5, 25, 3, '../trained_models')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9593a90b-e8ee-4893-8095-a006a8c2d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#New test split\n",
    "num_data = len(finetune_dataset)\n",
    "batch_size = 100\n",
    "new_trainval_dataset, new_test_dataset = torch.utils.data.random_split(finetune_dataset,  \n",
    "                                                            [int(0.5 * num_data), num_data - int(0.5 * num_data)])\n",
    "new_train_dataset, new_val_dataset = torch.utils.data.random_split(new_trainval_dataset,\n",
    "                                                           [int(0.8 * len(new_trainval_dataset)), len(new_trainval_dataset) - int(0.8 * len(new_trainval_dataset))])\n",
    "\n",
    "new_train_loader = DataLoader(new_train_dataset, shuffle=True, drop_last=True,\n",
    "                          batch_size=batch_size)\n",
    "new_val_loader = DataLoader(new_val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "new_test_loader = DataLoader(new_test_dataset, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53e0407f-da01-456a-99c4-ac5edb81a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 1000/1000 [46:56<00:00,  2.82s/it, loss=-75.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805\n",
      "Precision: 0.8068669527896996\n",
      "Recall: 0.7817047817047817\n",
      "F1-score: 0.7940865892291448\n"
     ]
    }
   ],
   "source": [
    "#Train Final Model\n",
    "\n",
    "trained_concept_informed_Pretrain_model.get_backbone()\n",
    "final_model = ConceptCLSUP_Finetune_Sc2(trained_concept_informed_Pretrain_model.get_backbone(), transform_dim, hidden_dim, embed_dim).to(device)\n",
    "final_model = train_fine_tune_model_sc2(final_model, new_train_loader, new_val_loader, 1000, 1e-5, 1e-5,'cuda', 100, 100)\n",
    "\n",
    "test_embeds = torch.stack([sample[0] for sample in  new_test_dataset]).to(device)\n",
    "test_concepts = torch.tensor([sample[1].item() for sample in  new_test_dataset]).unsqueeze(1).to(device)\n",
    "test_labels = torch.tensor([sample[-1].item() for sample in  new_test_dataset]).to(device)\n",
    "\n",
    "predictions = []\n",
    "idx = 0\n",
    "while idx+batch_size < len(test_embeds)+1:\n",
    "    _, out = final_model(test_embeds[idx:idx+batch_size], test_concepts[idx:idx+batch_size])\n",
    "    idx = idx+batch_size\n",
    "    predictions.append(torch.sigmoid(out).round().detach().cpu().numpy())\n",
    "        \n",
    "predictions = np.reshape(np.array(predictions), test_labels.shape)\n",
    "accuracy = accuracy_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "precision = precision_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "recall = recall_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "f1 = f1_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb2c4c-cb3b-4d83-96ee-7cda51c7b503",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline (Pre-Training with Concepts $x \\rightarrow c_1, x \\rightarrow y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e24d089-bb3f-445b-a17a-66ce1579d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [15:56<1:03:45,  4.78s/it, loss=0.000126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [01:47<07:09,  1.86it/s, loss=2.59e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy: 0.876\n",
      "Precision: 0.8872017353579176\n",
      "Recall: 0.8503118503118503\n",
      "F1-score: 0.8683651804670913\n"
     ]
    }
   ],
   "source": [
    "# train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True,\n",
    "#                           batch_size=batch_size)\n",
    "# val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "# test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False)\n",
    "\n",
    "backbone = mlp(transform_dim, 512, 1, layers=3, activation='relu')\n",
    "trained_backbone = mlp_train_c(backbone, train_loader, val_loader, 1000, 1e-5, 1e-5,'cuda', 100, 100)\n",
    "FC = mlp(1, 256, 1, 1, activation= 'relu')\n",
    "model = nn.Sequential(trained_backbone, FC)\n",
    "trained_model = mlp_train(model, new_train_loader, new_val_loader, 1000, 1e-5, 1e-5,'cuda', 100, 100)\n",
    "\n",
    "test_embeds = torch.stack([sample[0] for sample in  new_test_dataset]).detach().cpu().numpy()\n",
    "out = trained_model(torch.tensor(test_embeds).to(device))\n",
    "predictions = torch.sigmoid(out).round().detach().cpu().numpy()\n",
    "        \n",
    "accuracy = accuracy_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "precision = precision_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "recall = recall_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "f1 = f1_score(test_labels.detach().cpu().numpy(), predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2670c3-8db9-44d7-a890-453bcc443071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb81ba-d2ee-4baf-8c3a-7fbb5d56db10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f8654e-4800-4dfb-85e0-65a545034846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
