{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b1b8d8-b6c1-4a4d-8531-bcb516ed4c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb2e1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1909f81770>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from dataset import*\n",
    "from baselines import*\n",
    "from synthetic_concept_model import *\n",
    "from synthetic_coop_model import *\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "import torch\n",
    "import numpy\n",
    "import random\n",
    "import os\n",
    "\n",
    "random.seed(7)\n",
    "numpy.random.seed(seed=7)\n",
    "torch.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479b8138-eebc-41d8-a4a6-bf778329e14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !nvidia-smi\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454ff7e",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3a5653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generated data : 0\n",
      "Current generated data : 100\n",
      "Current generated data : 200\n",
      "Current generated data : 300\n",
      "Current generated data : 400\n",
      "Current generated data : 500\n",
      "Current generated data : 600\n",
      "Current generated data : 700\n",
      "Current generated data : 800\n",
      "Current generated data : 900\n"
     ]
    }
   ],
   "source": [
    "feature_dim_info = dict()\n",
    "label_dim_info = dict()\n",
    "transform_dim = 10000\n",
    "\n",
    "intersections = get_intersections(num_modalities=2)\n",
    "\n",
    "feature_dim_info['12'] = 10\n",
    "feature_dim_info['1'] = 6\n",
    "feature_dim_info['2'] = 6\n",
    "\n",
    "label_dim_info['12'] = 10\n",
    "label_dim_info['1'] = 6\n",
    "label_dim_info['2'] = 6\n",
    "num_concepts = 1\n",
    "transforms_2concept = None\n",
    "transforms_2hd = None\n",
    "num_data = 1000\n",
    "noise=0.3\n",
    "pos_prob=0.5\n",
    "total_data, total_labels, total_concepts, total_raw_features = generate_data_concepts(num_data, num_concepts,\n",
    "                                                                                      feature_dim_info,\n",
    "                                                                                      label_dim_info,\n",
    "                                                                                      transform_dim=transform_dim,\n",
    "                                                                                     noise=noise,\n",
    "                                                                                     pos_prob=pos_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9937f435-a421-4679-9ec7-df4c4779709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_raw_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4059c6-b3f0-4fc7-a4c4-291f4e518a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting & loading\n",
    "# dataset = MultiConcept(total_data, total_labels, total_concepts, 0)\n",
    "dataset = MultiConcept_w_Features(total_data, total_labels, total_concepts, 0, total_raw_features)\n",
    "\n",
    "batch_size = 100\n",
    "trainval_dataset, test_dataset = torch.utils.data.random_split(dataset,  \n",
    "                                                            [int(0.7 * num_data), num_data - int(0.7 * num_data)])\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset,\n",
    "                                                           [int(0.8 * len(trainval_dataset)), len(trainval_dataset) - int(0.8 * len(trainval_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31d4dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a388f33-7b36-4909-9e60-3b3293251cd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scenario 1 \n",
    "One known concept $c_1$ derived from information components $W_{U_1}, W_s$. Label $Y$ is composed of information components $y=f(W_{U_1}, W_s, W_{U_2})$. We try to recover $W_{U_2}$ by $\\arg \\max_{Z_x} I(Z_x;Y|Z_{c_1})$, assuming that $Z_{c_1}$ represents $\\{W_{U_1}, W_s\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53f075e2-48aa-452c-8e66-aad5ee3a62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "!!! CHANGE THE train_concept_encoder AND  train_concept_informed_model FUNCTIONS IF NEEDED!!!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def factorCBM_exp1(train_dataset, test_dataset,  num_eval=10, save_path='./results'): #concept_encoder, model, device,\n",
    "\n",
    "    # models\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    hidden_dim = 512\n",
    "    embed_dim = 50\n",
    "  \n",
    "    acc_list, pre_list, recall_list, f1_list = [], [], [], []\n",
    "    # teval = tqdm(range(num_eval))\n",
    "    for idx in range(num_eval):\n",
    "        concept_encoder = ConceptEncoder(transform_dim, embed_dim, 1, hidden_dim).to(device)\n",
    "        model = ConceptCLSUP_full_concept(transform_dim, embed_dim, 2, hidden_dim, embed_dim).to(device) #ConceptCLSUP_full_concept\n",
    "        train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True,\n",
    "                          batch_size=batch_size)\n",
    "        val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False)\n",
    "        # train concpet encoder\n",
    "        trained_concept_encoder = train_concept_encoder(concept_encoder, train_loader,val_loader, 1000, device, 1e-5, 1e-5, 25, 3, '../trained_models')\n",
    "        # train concept informed model\n",
    "        trained_concept_informed_model = train_concept_informed_model(trained_concept_encoder, model, train_loader, val_loader, 1000, device, 1e-5, 25, 3, '../trained_models') #train_concept_informed_model\n",
    "        \n",
    "        train_embeds_1 = trained_concept_encoder.get_embedding(torch.stack([sample[0] for sample in  train_dataset]).to(device)).detach().cpu().numpy() #torch.stack([sample[1] for sample in  train_dataset]).to(device).detach().cpu().numpy()#\n",
    "        train_embeds_2 = trained_concept_informed_model.get_embedding(torch.stack([sample[0] for sample in  train_dataset]).to(device)).detach().cpu().numpy()\n",
    "        train_embeds = np.concatenate((train_embeds_1, train_embeds_2), axis=1) #train_embeds_2 #\n",
    "        train_labels = np.array([sample[2].item() for sample in  train_dataset])\n",
    "\n",
    "        test_embeds_1 = trained_concept_encoder.get_embedding(torch.stack([sample[0] for sample in  test_dataset]).to(device)).detach().cpu().numpy() #torch.stack([sample[1] for sample in  test_dataset]).to(device).detach().cpu().numpy()#\n",
    "        test_embeds_2 = trained_concept_informed_model.get_embedding(torch.stack([sample[0] for sample in  test_dataset]).to(device)).detach().cpu().numpy()\n",
    "        test_embeds = np.concatenate((test_embeds_1, test_embeds_2), axis=1) #test_embeds_2 #\n",
    "        test_labels = np.array([sample[2].item() for sample in  test_dataset])\n",
    "\n",
    "        clf = LogisticRegression(max_iter=1000).fit(train_embeds, train_labels)\n",
    "        predictions = clf.predict(test_embeds)\n",
    "        \n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        precision = precision_score(test_labels, predictions)\n",
    "        recall = recall_score(test_labels, predictions)\n",
    "        f1 = f1_score(test_labels, predictions)\n",
    "        \n",
    "        acc_list.append(accuracy)\n",
    "        pre_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    dict_results = {'accuracy':acc_list,\n",
    "                    'precision':pre_list,\n",
    "                    'recall':recall_list,\n",
    "                    'f1_score':f1_list}\n",
    "    df_results = pd.DataFrame(data=dict_results)\n",
    "    \n",
    "    print(f'Accuracy:{df_results.accuracy.mean():0.3f} \\u00B1 {2*df_results.accuracy.std():0.3f}')\n",
    "    print(f'Precision:{df_results.precision.mean():0.3f} \\u00B1 {2*df_results.precision.std():0.3f}')\n",
    "    print(f'Recall:{df_results.recall.mean():0.3f} \\u00B1 {2*df_results.recall.std():0.3f}')\n",
    "    print(f'F1-score:{df_results.f1_score.mean():0.3f} \\u00B1 {2*df_results.f1_score.std():0.3f}')\n",
    "    \n",
    "    directory = save_path + '/' + time.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    timestr = time.strftime(\"%H%M%S\")\n",
    "    file_path = directory +  '/factorCBM_exp1_'+ str(num_eval) + '_' + timestr +'.csv'\n",
    "    df_results.to_csv(file_path)\n",
    "    \n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e160d497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc3fe8f9-169d-4459-a7ca-508a85183469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<01:04, 15.35it/s, loss=2.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39:   4%|▍         | 39/1000 [00:08<03:17,  4.87it/s, loss=-.0142]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<01:01, 16.24it/s, loss=2.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39:   4%|▍         | 39/1000 [00:07<03:05,  5.17it/s, loss=-.0153]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<01:21, 12.21it/s, loss=2.5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:01<03:48,  4.34it/s, loss=-6.73e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<00:59, 16.76it/s, loss=2.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   5%|▌         | 51/1000 [00:09<03:04,  5.15it/s, loss=-.0516]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<00:55, 17.97it/s, loss=2.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:   1%|          | 12/1000 [00:02<03:38,  4.52it/s, loss=-.000131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<00:55, 17.76it/s, loss=2.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   5%|▌         | 51/1000 [00:09<03:02,  5.20it/s, loss=-.0455]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<01:07, 14.65it/s, loss=2.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54:   5%|▌         | 54/1000 [00:10<03:04,  5.14it/s, loss=-.0633]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<00:51, 19.22it/s, loss=2.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57:   6%|▌         | 57/1000 [00:10<03:00,  5.22it/s, loss=-.0722]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<00:59, 16.65it/s, loss=2.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60:   6%|▌         | 60/1000 [00:11<03:03,  5.13it/s, loss=-.111]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<01:01, 16.24it/s, loss=1.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51:   5%|▌         | 51/1000 [00:09<03:02,  5.20it/s, loss=-.054]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy:0.736 ± 0.129\n",
      "Precision:0.717 ± 0.133\n",
      "Recall:0.793 ± 0.110\n",
      "F1-score:0.750 ± 0.083\n"
     ]
    }
   ],
   "source": [
    "results = factorCBM_exp1(train_dataset, test_dataset, num_eval=10, save_path='./results') #concept_encoder, model, device, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e909336-9d93-408c-9fd8-ff142887a7bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scenario 2\n",
    "\n",
    "Use both the supervised loss and the Info_club constraint to minimize the mutual information between the learned representation and c, then concatenate c to the learned representation and train everything end-to-end.\n",
    "\n",
    "$\\mathop{\\arg \\min}\\limits_{\\theta , \\phi} \\mathcal{L}\\bigl( y, f_{\\theta}(g_{\\phi}(x),c)\\bigr) + \\lambda Info_{NCE\\_CLUB} \\bigl( g_{\\phi}(x);c \\bigr)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58eedeec-dc27-49bc-a6d5-cea36af8a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "!!! CHANGE THE train_concept_encoder AND  train_concept_informed_model FUNCTIONS IF NEEDED!!!\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "class ConceptCLSUP_Sc2(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim, embed_dim, layers=2, activation='relu', lr=1e-4, concept_dim = 50):\n",
    "        super(ConceptCLSUP_Sc2, self).__init__()\n",
    "        self.critic_hidden_dim = 512\n",
    "        self.critic_layers = 1\n",
    "        self.critic_activation = 'relu'\n",
    "\n",
    "        # encoders\n",
    "        self.backbone = mlp(x_dim, hidden_dim, embed_dim, layers, activation)\n",
    "        self.linears_infonce = mlp(embed_dim, embed_dim, embed_dim, 1, activation) \n",
    "        self.y_projection = mlp(embed_dim + concept_dim, embed_dim, 1, 1, activation)\n",
    "\n",
    "        # critics\n",
    "        self.club_critic = CLUBInfoNCECritic(embed_dim , concept_dim, self.critic_hidden_dim, self.critic_layers, self.critic_activation) #MINECritic(\n",
    "        \n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # compute embedding\n",
    "        z = self.linears_infonce(self.backbone(x))\n",
    "        # compute critic scores\n",
    "        club_infonce_score = self.club_critic(z, c) #self.club_critic(torch.cat([z, x], dim=-1), c)self.club_critic(torch.cat([z, x], dim=-1), c) #\n",
    "        y_encoding = self.y_projection(torch.cat([z,c], dim=-1))\n",
    "        \n",
    "        return club_infonce_score, y_encoding \n",
    "\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def get_logits(self, x, c):\n",
    "        z = self.linears_infonce(self.backbone(x))\n",
    "        return self.y_projection(torch.cat([z,c], dim=-1))\n",
    "    \n",
    "    def get_backbone(self):\n",
    "        return self.backbone\n",
    "    \n",
    "    \n",
    "def train_concept_informed_model_sc2(concept_encoder, model, train_loader,val_loader, num_epochs, device, lr, lamb, log_interval,\n",
    "                          save_interval, save_path):\n",
    "\n",
    "    # concept_encoder.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    label_loss_func = torch.nn.BCELoss()\n",
    "    best_val_err = torch.tensor(1e7)\n",
    "    tepoch = tqdm(range(num_epochs))\n",
    "    # print(f'lamda {lamb}')\n",
    "    for epoch in tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        for batch_idx, (data, concept, target,_) in enumerate(train_loader):\n",
    "            data, concept, target = data.to(device), concept.to(device) ,target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            c, z_c = concept_encoder(data)\n",
    "            MI_loss, y_logits = model(data, z_c) #concept\n",
    "            label_loss = label_loss_func(torch.sigmoid(y_logits), target.float())\n",
    "            # print(f'label loss {label_loss}, MI loss {MI_loss}')\n",
    "            loss = label_loss + lamb * MI_loss #\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # torch.cuda.empty_cache()\n",
    "            # gc.collect()\n",
    "            \n",
    "        tepoch.set_postfix(loss=loss.item())\n",
    "        if epoch % save_interval == 0:\n",
    "            val_err = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, concept, target,_) in enumerate(val_loader):\n",
    "                    data, concept, target = data.to(device), concept.to(device), target.to(device)\n",
    "                    c , z_c = concept_encoder(data)\n",
    "                    # output = model(data, c, target) #z_c\n",
    "                    MI_loss, y_logits = model(data, z_c)#concept\n",
    "                    label_loss = label_loss_func(torch.sigmoid(y_logits), target.float())\n",
    "                    loss = label_loss + lamb * MI_loss \n",
    "                    val_err += loss\n",
    "                val_err = val_err / len(val_loader)\n",
    "            if val_err < best_val_err:\n",
    "                best_val_err = val_err\n",
    "\n",
    "            else:\n",
    "                print('Val loss did not improve')\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'concept_informed_model.pth'))\n",
    "                return model\n",
    "    return model\n",
    "\n",
    "def factorCBM_exp1_Sc2(train_dataset, test_dataset, num_eval=10, save_path='./results'): #, concept_encoder, model, device,\n",
    "\n",
    "    acc_list, pre_list, recall_list, f1_list = [], [], [], []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    hidden_dim = 512\n",
    "    embed_dim = 256\n",
    "    \n",
    "    # teval = tqdm(range(num_eval))\n",
    "    for idx in range(num_eval):\n",
    "        concept_encoder = ConceptEncoder(transform_dim, embed_dim, 1, hidden_dim).to(device)\n",
    "        model = ConceptCLSUP_Sc2(transform_dim, hidden_dim, embed_dim, 2, 'relu').to(device)\n",
    "        train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True,\n",
    "                          batch_size=batch_size)\n",
    "        val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False)\n",
    "        # train concpet encoder\n",
    "        trained_concept_encoder = train_concept_encoder(concept_encoder, train_loader,val_loader, 1000, device, 1e-5, 1e-5, 25, 3, '../trained_models')\n",
    "        # train concept informed model concept_encoder\n",
    "        trained_concept_informed_model = train_concept_informed_model_sc2(trained_concept_encoder, model, train_loader, val_loader, num_epochs=1000, device=device,\n",
    "                                                                          lr=1e-5, lamb=0.5, log_interval = 25, save_interval = 3, save_path ='../trained_models')\n",
    "        \n",
    "        test_embeds =  torch.stack([sample[0] for sample in  test_dataset]).to(device)\n",
    "        test_concepts = torch.tensor([sample[1] for sample in  test_dataset]).unsqueeze(1).to(device)\n",
    "        test_labels = np.array([sample[2].item() for sample in  test_dataset])\n",
    "\n",
    "        \n",
    "        out = trained_concept_informed_model.get_logits(test_embeds, trained_concept_encoder.get_embedding(test_embeds)) #test_concepts\n",
    "        \n",
    "        predictions = torch.sigmoid(out).round().detach().cpu().numpy()\n",
    "        \n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        precision = precision_score(test_labels, predictions)\n",
    "        recall = recall_score(test_labels, predictions)\n",
    "        f1 = f1_score(test_labels, predictions)\n",
    "        \n",
    "        acc_list.append(accuracy)\n",
    "        pre_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "    \n",
    "    dict_results = {'accuracy':acc_list,\n",
    "                    'precision':pre_list,\n",
    "                    'recall':recall_list,\n",
    "                    'f1_score':f1_list}\n",
    "    df_results = pd.DataFrame(data=dict_results)\n",
    "    \n",
    "    print(f'Accuracy:{df_results.accuracy.mean():0.3f} \\u00B1 {2*df_results.accuracy.std():0.3f}')\n",
    "    print(f'Precision:{df_results.precision.mean():0.3f} \\u00B1 {2*df_results.precision.std():0.3f}')\n",
    "    print(f'Recall:{df_results.recall.mean():0.3f} \\u00B1 {2*df_results.recall.std():0.3f}')\n",
    "    print(f'F1-score:{df_results.f1_score.mean():0.3f} \\u00B1 {2*df_results.f1_score.std():0.3f}')\n",
    "    \n",
    "    directory = save_path + '/' + time.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    timestr = time.strftime(\"%H%M%S\")\n",
    "    file_path = directory +  '/factorCBM_exp1_Sc2_'+ str(num_eval) + '_' + timestr +'.csv'\n",
    "    df_results.to_csv(file_path)\n",
    "    \n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d678604-9f44-4a0a-a915-91e2c9769cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b67dfd6-c1de-4f86-87c0-90749b2fbb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<00:58, 16.99it/s, loss=2.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<02:13,  7.44it/s, loss=0.702]\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<01:01, 16.15it/s, loss=2.37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<02:16,  7.31it/s, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<01:01, 16.16it/s, loss=2.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<02:39,  6.27it/s, loss=0.691]\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<00:57, 17.23it/s, loss=2.4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:   1%|          | 12/1000 [00:01<01:39,  9.96it/s, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<00:58, 16.91it/s, loss=2.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<02:21,  7.03it/s, loss=0.692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<00:56, 17.68it/s, loss=2.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<02:20,  7.11it/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<01:15, 13.14it/s, loss=2.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<02:00,  8.24it/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<00:53, 18.42it/s, loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:01<01:55,  8.57it/s, loss=0.701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:   1%|          | 9/1000 [00:00<00:55, 17.73it/s, loss=2.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<02:10,  7.63it/s, loss=0.691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<01:30, 10.93it/s, loss=2.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:00<02:11,  7.53it/s, loss=0.688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy:0.496 ± 0.011\n",
      "Precision:0.395 ± 0.416\n",
      "Recall:0.800 ± 0.843\n",
      "F1-score:0.529 ± 0.557\n"
     ]
    }
   ],
   "source": [
    "results = factorCBM_exp1_Sc2(train_dataset, test_dataset, num_eval=10, save_path='./results') #concept_encoder, model, device,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575e2f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline 1 (logistic regression on $x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef7c9952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation 9: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.757 ± 0.000\n",
      "Precision:0.739 ± 0.000\n",
      "Recall:0.784 ± 0.000\n",
      "F1-score:0.761 ± 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_1 = baseline_1(train_dataset, test_dataset, num_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5d0a5-03be-402e-9a56-c3f50811e89f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline 2 (Supervised Representation Learning on $x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6046bbeb-7b24-47c3-9d94-1c5af8c0f90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.18it/s, loss=0.000127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy:0.777 ± nan\n",
      "Precision:0.745 ± nan\n",
      "Recall:0.831 ± nan\n",
      "F1-score:0.786 ± nan\n"
     ]
    }
   ],
   "source": [
    "results_2 = baseline_2(train_dataset, val_dataset, test_dataset, transform_dim=transform_dim, batch_size=batch_size, num_eval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7c1c8-721f-4eca-a571-73a29d83a521",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18f13c-0240-4a48-ac24-419fdc1900c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Logistic Regression on $x,c_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b94ad4d0-8c54-4dcf-a4dd-b4aa6c4fe866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation 9: 100%|██████████| 10/10 [00:12<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.860 ± 0.000\n",
      "Precision:0.849 ± 0.000\n",
      "Recall:0.872 ± 0.000\n",
      "F1-score:0.860 ± 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resuts_3_A = baseline_3_A(train_dataset, test_dataset, num_eval=10, save_path='./results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94feeba-44bd-4ffb-98d1-8c419385b4e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (Supervised Representation Learning on $x,c_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f184415-dec1-4ab9-bba8-7120f30de237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:38, 21.03it/s, loss=0.000136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:32, 21.64it/s, loss=4.51e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.75it/s, loss=0.000125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.53it/s, loss=0.00011] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.47it/s, loss=0.000137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.44it/s, loss=0.000141]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100:  10%|█         | 100/1000 [00:04<00:42, 21.11it/s, loss=0.000795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.52it/s, loss=0.000131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:32, 21.76it/s, loss=4.17e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 400:  40%|████      | 400/1000 [00:18<00:27, 21.50it/s, loss=2.12e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy:0.783 ± 0.015\n",
      "Precision:0.766 ± 0.016\n",
      "Recall:0.808 ± 0.028\n",
      "F1-score:0.786 ± 0.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_3_b = baseline_3_B(train_dataset, val_dataset, test_dataset, transform_dim=transform_dim, batch_size=batch_size, num_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1e358-1f82-4801-af13-d05a847ce8a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline 4 (Multi-Task Learning with Concepts $x \\rightarrow y, c_1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5037329a-68f2-48a3-acd1-1f4858b5d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:08<00:35, 22.34it/s, loss=0.0022] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:08<00:35, 22.25it/s, loss=0.00223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:31, 22.30it/s, loss=0.000586]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:08<00:35, 22.33it/s, loss=0.00214]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:31, 22.42it/s, loss=0.000607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:08<00:35, 22.29it/s, loss=0.00203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:31, 22.31it/s, loss=0.00065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:31, 22.20it/s, loss=0.000593]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:08<00:35, 22.38it/s, loss=0.00235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:08<00:35, 22.43it/s, loss=0.00226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy:0.772 ± 0.018\n",
      "Precision:0.755 ± 0.023\n",
      "Recall:0.796 ± 0.019\n",
      "F1-score:0.775 ± 0.016\n"
     ]
    }
   ],
   "source": [
    "results_4 = baseline_4(train_dataset, val_dataset, test_dataset, transform_dim=transform_dim, batch_size=batch_size, num_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6e45c-2ffe-4947-986a-0602d8f5d0e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline 5 (Pre-Training with Concepts $x \\rightarrow c_1, x \\rightarrow y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b912cd7-42d6-43fa-8424-671e08e2641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 22.00it/s, loss=1.39e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.43it/s, loss=0.000189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.71it/s, loss=2.49e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.63it/s, loss=0.000215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.85it/s, loss=4.07e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.49it/s, loss=0.000185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:31, 21.97it/s, loss=1.72e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.62it/s, loss=0.000169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.93it/s, loss=6.83e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.31it/s, loss=0.00022] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.87it/s, loss=6.21e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.72it/s, loss=0.000158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300:  30%|███       | 300/1000 [00:13<00:32, 21.82it/s, loss=1.11e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.48it/s, loss=0.000257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.92it/s, loss=1.68e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100:  10%|█         | 100/1000 [00:04<00:42, 21.21it/s, loss=0.00121]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.84it/s, loss=6.76e-11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:36, 21.78it/s, loss=0.000246]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 400:  40%|████      | 400/1000 [00:18<00:27, 21.85it/s, loss=1.24e-10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200:  20%|██        | 200/1000 [00:09<00:37, 21.47it/s, loss=0.000203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy:0.772 ± 0.011\n",
      "Precision:0.755 ± 0.013\n",
      "Recall:0.795 ± 0.018\n",
      "F1-score:0.775 ± 0.011\n"
     ]
    }
   ],
   "source": [
    "results_5 = baseline_5(train_dataset, val_dataset, test_dataset, transform_dim=transform_dim, batch_size=batch_size, num_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24b271-1825-48da-85d6-fc0670ce8a31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hard conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdd315df-0f81-41d0-82e7-ab513d637ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kmeans-pytorch\n",
      "  Downloading kmeans_pytorch-0.3-py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: kmeans-pytorch\n",
      "Successfully installed kmeans-pytorch-0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install kmeans-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1cf7c279-2f99-441a-a97f-4290ad817965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans, kmeans_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d8995b9f-53c0-40f3-b603-748f0bb6ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConceptCLSUP_Hard_Cond(nn.Module):\n",
    "#     def __init__(self, x_dim, hidden_dim, embed_dim, bin_dim, layers=2, activation='relu', lr=1e-4):\n",
    "#         super(ConceptCLSUP_Hard_Cond, self).__init__()\n",
    "#         self.critic_hidden_dim = 512\n",
    "#         self.critic_layers = 1\n",
    "#         self.critic_activation = 'relu'\n",
    "#         self.lr = lr\n",
    "\n",
    "#         # encoders\n",
    "#         self.backbone = mlp(x_dim, hidden_dim, embed_dim, layers, activation)\n",
    "#         self.linears_infonce = mlp(embed_dim, embed_dim, embed_dim, 1, activation) \n",
    "\n",
    "#         # critics\n",
    "#         concept_dim = 1\n",
    "#         self.club_critic = CLUBInfoNCECritic(embed_dim + x_dim, concept_dim, self.critic_hidden_dim, self.critic_layers, self.critic_activation)\n",
    "\n",
    "#     def forward(self, x, c, sliced_x):\n",
    "#         # compute embedding\n",
    "#         z = self.linears_infonce(self.backbone(x))\n",
    "#         # compute critic scores\n",
    "#         # print(f'size of now {x[:,curr_idx:curr_idx+bin_dim].shape}, actual size {embed_dim + bin_dim}')\n",
    "#         # print(f'size of z {z.shape}')\n",
    "#         club_infonce_score = self.club_critic(torch.cat([z, sliced_x], dim=-1), c) #\n",
    "#         return club_infonce_score\n",
    "\n",
    "#     def get_embedding(self, x):\n",
    "#         return self.backbone(x)\n",
    "    \n",
    "#     def get_backbone(self):\n",
    "#         return self.backbone\n",
    "    \n",
    "\n",
    "def train_concept_informed_HC(concept_encoder, model, train_loader,val_loader, num_epochs, cluster_centers, device, lr, log_interval,\n",
    "                          save_interval, save_path):\n",
    "\n",
    "    concept_encoder.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_val_err = torch.tensor(1e7)\n",
    "    tepoch = tqdm(range(num_epochs))\n",
    "    for epoch in tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, concept, target,_) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            cluster_ids = kmeans_predict(data, cluster_centers, distance='euclidean', device=device)\n",
    "            current_data = data[cluster_ids == batch_idx]\n",
    "            c, z_c = concept_encoder(current_data)\n",
    "            loss = model(current_data, c, target[cluster_ids == batch_idx])\n",
    "            \n",
    "            # bounds = torch.linspace(torch.min(data).item(), torch.max(data).item(), 6).to(device)\n",
    "            # ind_list = torch.bucketize(data, bounds) \n",
    "            # sliced_x = data.clone()\n",
    "            # sliced_x [ind_list != batch_idx] = 0\n",
    "            # c, z_c = concept_encoder(data)\n",
    "            # loss = model(sliced_x, c, target) #z_c\n",
    "             \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        tepoch.set_postfix(loss=loss.item())\n",
    "        if epoch % save_interval == 0:\n",
    "            val_err = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, concept, target,_) in enumerate(val_loader):\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    cluster_ids = kmeans_predict(data, cluster_centers, distance='euclidean', device=device)\n",
    "                    current_data = data[cluster_ids == batch_idx]\n",
    "                    c , z_c = concept_encoder(current_data)\n",
    "                    output = model(current_data, c, target[cluster_ids == batch_idx])\n",
    "                    \n",
    "                    # bounds = torch.linspace(torch.min(data).item(), torch.max(data).item(), 6).to(device)\n",
    "                    # ind_list = torch.bucketize(data, bounds) \n",
    "                    # sliced_x = data.clone()\n",
    "                    # sliced_x [ind_list != batch_idx] = 0\n",
    "                    # c , z_c = concept_encoder(data)\n",
    "                    # output = model(sliced_x, c, target) #z_c\n",
    "                    val_err += output\n",
    "                val_err = val_err / len(val_loader)\n",
    "            if val_err < best_val_err:\n",
    "                best_val_err = val_err\n",
    "\n",
    "            else:\n",
    "                print('Val loss did not improve')\n",
    "                torch.save(model.state_dict(), os.path.join(save_path, 'concept_informed_model.pth'))\n",
    "                return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6261541-1379-4b8f-9c79-8c103e9fef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 512\n",
    "embed_dim = 256\n",
    "concept_encoder = ConceptEncoder(transform_dim, embed_dim, 1, hidden_dim, layers=1).to(device)\n",
    "model = ConceptCLSUP_full_concept(transform_dim, embed_dim, 2, hidden_dim, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c2645f25-6716-446f-90cd-84eabc3a2902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda:0..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 3it [00:00,  7.88it/s, center_shift=0.000000, iteration=3, tol=0.000100]\n",
      "Epoch 12:   1%|          | 12/1000 [00:00<00:52, 18.89it/s, loss=0.00106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1000 [00:00<?, ?it/s, loss=-7.38e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 1/1000 [00:00<03:42,  4.48it/s, loss=-7.38e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 2/1000 [00:00<03:35,  4.63it/s, loss=-5.16e-5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<03:21,  4.95it/s, loss=-5.32e-8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<03:21,  4.95it/s, loss=7.04e-7] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "predicting on cuda..\n",
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 3/1000 [00:00<05:03,  3.29it/s, loss=7.04e-7]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False)\n",
    "cluster_ids_x, cluster_centers = kmeans(\n",
    "    X=torch.stack([sample[0] for sample in  train_dataset]), num_clusters=6, distance='euclidean', device=torch.device('cuda:0')\n",
    ")\n",
    "# train concpet encoder \n",
    "trained_concept_encoder = train_concept_encoder(concept_encoder, train_loader,val_loader, 1000, device, 1e-5, 1e-5, 25, 3, '../trained_models')\n",
    "# train concept informed model\n",
    "trained_concept_informed_HC_model = train_concept_informed_HC(trained_concept_encoder, model, train_loader, val_loader, 1000, cluster_centers, device, 1e-5, 25, 3, '../trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "67b9f2d2-31a8-48e0-b044-f6cd8c68877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6066666666666667\n",
      "Precision: 0.5666666666666667\n",
      "Recall: 0.9066666666666666\n",
      "F1-score: 0.6974358974358975\n"
     ]
    }
   ],
   "source": [
    "train_embeds_1 = trained_concept_encoder.get_embedding(torch.stack([sample[0] for sample in  train_dataset]).to(device)).detach().cpu().numpy()\n",
    "train_embeds_2 = trained_concept_informed_HC_model.get_embedding(torch.stack([sample[0] for sample in  train_dataset]).to(device)).detach().cpu().numpy()\n",
    "train_embeds = np.concatenate((train_embeds_1, train_embeds_2), axis=1) #train_embeds_2 #\n",
    "train_labels = np.array([sample[2].item() for sample in  train_dataset])\n",
    "\n",
    "test_embeds_1 = trained_concept_encoder.get_embedding(torch.stack([sample[0] for sample in  test_dataset]).to(device)).detach().cpu().numpy()\n",
    "test_embeds_2 = trained_concept_informed_HC_model.get_embedding(torch.stack([sample[0] for sample in  test_dataset]).to(device)).detach().cpu().numpy()\n",
    "test_embeds = np.concatenate((test_embeds_1, test_embeds_2), axis=1) #test_embeds_2 #\n",
    "test_labels = np.array([sample[2].item() for sample in  test_dataset])\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000).fit(train_embeds, train_labels)\n",
    "predictions = clf.predict(test_embeds)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ec5abc9a-ab38-4f6c-8269-0f1098a2bc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21:   2%|▏         | 21/1000 [00:01<00:50, 19.31it/s, loss=0.000469]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   1%|          | 6/1000 [00:01<05:28,  3.03it/s, loss=-.000234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss did not improve\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.6106194690265486\n",
      "Recall: 0.92\n",
      "F1-score: 0.7340425531914894\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "hidden_dim = 512\n",
    "embed_dim = 256\n",
    "concept_encoder = ConceptEncoder(transform_dim, embed_dim, 1, hidden_dim, layers=1).to(device)\n",
    "model = ConceptCLSUP_full_concept(transform_dim, embed_dim, 2, hidden_dim, embed_dim).to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False)\n",
    "# train concpet encoder \n",
    "trained_concept_encoder = train_concept_encoder(concept_encoder, train_loader,val_loader, 1000, device, 1e-5, 1e-5, 25, 3, '../trained_models')\n",
    "# train concept informed model\n",
    "trained_concept_informed_NC_model = train_concept_informed_model(trained_concept_encoder, model, train_loader, val_loader, 1000, device, 1e-5, 25, 3, '../trained_models')\n",
    "\n",
    "train_embeds_1 = trained_concept_encoder.get_embedding(torch.stack([sample[0] for sample in  train_dataset]).to(device)).detach().cpu().numpy()\n",
    "train_embeds_2 = trained_concept_informed_NC_model.get_embedding(torch.stack([sample[0] for sample in  train_dataset]).to(device)).detach().cpu().numpy()\n",
    "train_embeds = np.concatenate((train_embeds_1, train_embeds_2), axis=1) #train_embeds_2 #\n",
    "train_labels = np.array([sample[2].item() for sample in  train_dataset])\n",
    "\n",
    "test_embeds_1 = trained_concept_encoder.get_embedding(torch.stack([sample[0] for sample in  test_dataset]).to(device)).detach().cpu().numpy()\n",
    "test_embeds_2 = trained_concept_informed_NC_model.get_embedding(torch.stack([sample[0] for sample in  test_dataset]).to(device)).detach().cpu().numpy()\n",
    "test_embeds = np.concatenate((test_embeds_1, test_embeds_2), axis=1) #test_embeds_2 #\n",
    "test_labels = np.array([sample[2].item() for sample in  test_dataset])\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000).fit(train_embeds, train_labels)\n",
    "predictions = clf.predict(test_embeds)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "precision = precision_score(test_labels, predictions)\n",
    "recall = recall_score(test_labels, predictions)\n",
    "f1 = f1_score(test_labels, predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06c575-dc77-4e37-9529-666a6219ec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f641481-42dd-4398-af3d-65f6512d81b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
